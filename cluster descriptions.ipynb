{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering the Flickr30k descriptions\n",
    "\n",
    "The code below clusters the referring expressions in the Flickr30k Entities data set. It corresponds to the following algorithm:\n",
    "\n",
    "### Part 1: counting\n",
    "\n",
    "1. Start with an empty Counter to count the amount of times two expressions refer to the same entity.\n",
    "2. For each set of captions, extract the referring expressions. Then:\n",
    "    \n",
    "    a. For each entity, create a set of expressions (of size $n$) that refer to that entity.\n",
    "    \n",
    "    b. For each set of referring expressions, add all the $\\dbinom{n}{2}$ combinations of expressions to the counter.\n",
    "    \n",
    "**Result:** a Counter storing the amount of times each combination of expressions has occurred.\n",
    "\n",
    "### Part 2: clustering the data, forming partitions\n",
    "\n",
    "1. Create a new undirected graph G.\n",
    "2. For each combination of referring expressions, if that combination occurs more than $\\theta=1$ times, both referring expressions become nodes in G, connected by an edge.\n",
    "3. Perform (non-parametric) Louvain clustering on G, creating partitions.\n",
    "\n",
    "**Result:** Sets of descriptions, where each set corresponds to a cluster. Note that the union of these sets is a proper subset of the total set of descriptions.\n",
    "\n",
    "### Part 3: extending the clusters\n",
    "\n",
    "1. Create a cluster index, mapping each description to the ID number of their cluster.\n",
    "2. Cluster previously ignored referring expressions: for each unclustered expression, count the different clusters that it is linked to. Add it to the cluster that it is linked to the most. In case of a tie, there are two options: do nothing, or randomly add it to one of the clusters it is linked to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import iglob\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import community\n",
    "from networkx.readwrite import json_graph\n",
    "from math import log\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_desc_and_link_counters(files):\n",
    "    \"\"\"\n",
    "    Generates links between descriptions used to describe the same objects.\n",
    "    Get a counter for the descriptions, and a counter for the links.\n",
    "    \"\"\"\n",
    "    pattern = re.compile('\\[(.*?)\\]') # omitted question mark\n",
    "    description_counter = defaultdict(int)\n",
    "    link_counter = Counter()\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            categorized_descriptions = defaultdict(set)\n",
    "            text = f.read()\n",
    "            annotations = re.findall(pattern, text)\n",
    "            for annotation in annotations:\n",
    "                # Get the category and description from the annotation:\n",
    "                category, *description = annotation.split()\n",
    "                description = ' '.join(description).lower()\n",
    "                # Categorize descriptions:\n",
    "                categorized_descriptions[category].add(description)\n",
    "                description_counter[description] += 1\n",
    "            for descriptions in categorized_descriptions.values():\n",
    "                link_counter.update(tuple(sorted(pair)) \n",
    "                                    for pair in combinations(descriptions,2))\n",
    "    return description_counter, link_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def partitions_from_link_counter(link_counter, threshold=0):\n",
    "    \"\"\"\n",
    "    Function that takes a link counter, constructs a graph, applies louvain clustering,\n",
    "    and yields partitions from that clustering as sets. Links should occur more than\n",
    "    threshold times to be considered.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(pair for pair,count in link_counter.items() if count > threshold)\n",
    "    for sub in sorted(nx.connected_component_subgraphs(G), key=len, reverse=True):\n",
    "        partitioning = community.best_partition(sub)\n",
    "        partition_sets = defaultdict(set)\n",
    "        for node, partition in partitioning.items():\n",
    "            partition_sets[partition].add(node)\n",
    "        for s in partition_sets.values():\n",
    "            yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = iglob('./static/Flickr30kEntities/Sentences/*.txt')\n",
    "description_counter, link_counter = get_desc_and_link_counters(files)\n",
    "partition_sets = list(partitions_from_link_counter(link_counter, threshold=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extended_partition_sets(partition_sets, below_threshold, check=True):\n",
    "    \"\"\"\n",
    "    Function to extend the partitions created using the function above.\n",
    "    Adds descriptions to the partition they have the most connections with.\n",
    "    \"\"\"\n",
    "    item_to_num = {item: num for num, s in enumerate(partition_sets) for item in s}\n",
    "    in_network = set(item_to_num.keys())\n",
    "    belongs_with = defaultdict(Counter)\n",
    "    remainder = []\n",
    "    for a,b in below_threshold:\n",
    "        a_in = a in in_network\n",
    "        b_in = b in in_network\n",
    "        if a_in and b_in:\n",
    "            continue\n",
    "        elif a_in and not b_in:\n",
    "            belongs_with[b].update([item_to_num[a]])\n",
    "        elif b_in and not a_in:\n",
    "            belongs_with[a].update([item_to_num[b]])\n",
    "        else:\n",
    "            remainder.append((a,b))\n",
    "    for description, c in belongs_with.items():\n",
    "        first, *rest = c.most_common()\n",
    "        num, count = first\n",
    "        if check:\n",
    "            if len(rest) == 0:\n",
    "                item_to_num[description] = num\n",
    "            else:\n",
    "                second, *rest = rest\n",
    "                num2, count2 = second\n",
    "                if count > count2:\n",
    "                    item_to_num[description] = num\n",
    "        else:\n",
    "            item_to_num[description] = num\n",
    "    d = defaultdict(set)\n",
    "    for item, num in item_to_num.items():\n",
    "        d[num].add(item)\n",
    "    return d.values(), remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ; unclassified: 147510\n",
      "iteration 1 ; unclassified: 33201\n",
      "iteration 2 ; unclassified: 12436\n",
      "iteration 3 ; unclassified: 9833\n",
      "iteration 4 ; unclassified: 9497\n",
      "iteration 5 ; unclassified: 9436\n",
      "iteration 6 ; unclassified: 9428\n",
      "iteration 7 ; unclassified: 9428\n",
      "iteration 8 ; unclassified: 9428\n",
      "iteration 9 ; unclassified: 9428\n",
      "iteration 10 ; unclassified: 9428\n",
      "partition sets: 751\n"
     ]
    }
   ],
   "source": [
    "# Iteratively try to extend the partitions.\n",
    "below_threshold = {pair for pair,count in link_counter.items() if count == 1}\n",
    "print('iteration', 0, ';', 'unclassified:', len(below_threshold))\n",
    "for i in range(1,11):\n",
    "    partition_sets, below_threshold = extended_partition_sets(partition_sets, \n",
    "                                                              below_threshold,\n",
    "                                                              check=False)\n",
    "    print('iteration', i, ';', 'unclassified:', len(below_threshold))\n",
    "\n",
    "print('partition sets:', len(partition_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitions_in_order = sorted(partition_sets, key=len, reverse=True)\n",
    "for i, s in enumerate(partitions_in_order):\n",
    "    with open('clusters/' + str(i)+'.txt','w') as f:\n",
    "        f.write('\\n'.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732\n"
     ]
    }
   ],
   "source": [
    "# Cluster the remaining descriptions as much as possible.\n",
    "link_counter_rest = {pair:1 for pair in below_threshold}\n",
    "rest_partitions = [p for p in partitions_from_link_counter(link_counter, threshold=0)\n",
    "                   if len(p) > 3]\n",
    "print(len(rest_partitions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
